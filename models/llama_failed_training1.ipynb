{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## The finetuning failed due to memory and hardware issue\n",
    "- moved to Google Colab\n",
    "\n",
    "### Original Plan\n",
    "- Running Sampler Datasets through Llama to Generate New Donor Profiles\n",
    "\n",
    "Section 1: Setup and Imports \n",
    "- Imports\n",
    "- Tokenizer initialization\n",
    "- Path definitions\n",
    "\n",
    "Section 2: Data Processing \n",
    "- Profile formatting functions\n",
    "- Data processing and saving\n",
    "- Verification checks\n",
    "\n",
    "Section 3: Model Configuration \n",
    "- Model initialization\n",
    "- LoRA configuration\n",
    "- Training parameters\n",
    "\n",
    "Section 4: Training \n",
    "- Training loop\n",
    "- Progress tracking\n",
    "- Model saving\n",
    "\n",
    "Section 5: Generation and Evaluation\n",
    "- Profile generation\n",
    "- Comparison with other LLMs\n",
    "- Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS available: True\n",
      "PyTorch version: 2.5.1\n",
      "Using MPS device: mps\n"
     ]
    }
   ],
   "source": [
    "## Verifying MPS availability\n",
    "\n",
    "import torch\n",
    "print(\"MPS available:\", torch.backends.mps.is_available())\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Using MPS device:\", torch.device(\"mps\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from peft) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from peft) (24.2)\n",
      "Requirement already satisfied: psutil in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from peft) (2.5.1)\n",
      "Requirement already satisfied: transformers in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from peft) (4.46.2)\n",
      "Requirement already satisfied: tqdm in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from peft) (4.67.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from peft) (1.1.1)\n",
      "Requirement already satisfied: safetensors in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from peft) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from peft) (0.26.2)\n",
      "Requirement already satisfied: filelock in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft) (2024.9.0)\n",
      "Requirement already satisfied: requests in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from torch>=1.13.0->peft) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from transformers->peft) (0.20.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (0.2.0)\n",
      "Requirement already satisfied: bitsandbytes in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (0.42.0)\n",
      "Requirement already satisfied: scipy in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from scipy->bitsandbytes) (2.0.2)\n",
      "Requirement already satisfied: accelerate in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (1.1.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from accelerate) (0.26.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from accelerate) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from accelerate) (2.5.1)\n",
      "Requirement already satisfied: filelock in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.9.0)\n",
      "Requirement already satisfied: requests in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n",
    "!pip install bitsandbytes\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"mytokenhere\")  \n",
    "\n",
    "# My real token is removed prior to submitting this file for hand-in for security purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    LlamaTokenizer,\n",
    "    LlamaForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Let's assume we're working with your original dataset first\n",
    "donor_data = load_dataset('/Users/cindylinsf/Documents/CCI/THESIS/Msc_Thesis_Project_Files/data/sampled/sampled_datasets/original_sample_20241111_231039.json')\n",
    "\n",
    "# Format each profile into instruction format\n",
    "def prepare_donor_profile(profile):\n",
    "    formatted_profile = f\"\"\"### Instruction:\n",
    "Generate a detailed sperm donor profile based on these characteristics:\n",
    "Height: {profile['height']}\n",
    "Weight: {profile['weight']}\n",
    "Eye Color: {profile['eye_color'] or 'Not specified'}\n",
    "Hair: {profile['hair_color'] or 'Not specified'}\n",
    "Education: {profile['education_level']} in {profile['education_field']}\n",
    "Ethnic Background: {profile['ethnic_background']}\n",
    "\n",
    "### Response:\n",
    "{profile['donor_description']}\n",
    "\n",
    "### End\"\"\"\n",
    "    return formatted_profile\n",
    "\n",
    "# Convert all profiles\n",
    "formatted_data = [prepare_donor_profile(profile) for profile in donor_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting protobuf\n",
      "  Downloading protobuf-5.28.3-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Downloading protobuf-5.28.3-cp38-abi3-macosx_10_9_universal2.whl (414 kB)\n",
      "Installing collected packages: protobuf\n",
      "Successfully installed protobuf-5.28.3\n"
     ]
    }
   ],
   "source": [
    "!pip install protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from transformers import LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "original_dataset_path = \"/Users/cindylinsf/Documents/CCI/THESIS/Msc_Thesis_Project_Files/data/sampled/sampled_datasets/original_sample_20241111_231039.json\"\n",
    "augmented_dataset_path = \"/Users/cindylinsf/Documents/CCI/THESIS/Msc_Thesis_Project_Files/data/sampled/sampled_datasets/augmented_sample_20241111_231039.json\"\n",
    "\n",
    "# Define output directory\n",
    "processed_dir = \"/Users/cindylinsf/Documents/CCI/THESIS/Msc_Thesis_Project_Files/data/processed/llama_training\"\n",
    "if not os.path.exists(processed_dir):\n",
    "    os.makedirs(processed_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the profile preparation function\n",
    "def prepare_donor_profile(profile):\n",
    "    \"\"\"Format a single donor profile into instruction format\"\"\"\n",
    "    formatted_profile = f\"\"\"### Instruction:\n",
    "Generate a detailed sperm donor profile based on these characteristics:\n",
    "Height: {profile['height']}\n",
    "Weight: {profile['weight']}\n",
    "Eye Color: {profile['eye_color']}\n",
    "Hair: {profile['hair_color']}\n",
    "Education: {profile['education_level']} in {profile['education_field']}\n",
    "Ethnic Background: {profile['ethnic_background']}\n",
    "\n",
    "### Response:\n",
    "{profile['donor_description']}\n",
    "\n",
    "### End\"\"\"\n",
    "    return formatted_profile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the processing function\n",
    "def load_and_process_dataset(input_file_path, dataset_type):\n",
    "    \"\"\"\n",
    "    Load and process a dataset, saving the results\n",
    "    dataset_type: 'original' or 'augmented'\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    with open(input_file_path, 'r') as f:\n",
    "        donor_data = json.load(f)\n",
    "    \n",
    "    # Process each profile\n",
    "    processed_data = []\n",
    "    for profile in donor_data:\n",
    "        formatted_profile = prepare_donor_profile(profile)\n",
    "        \n",
    "        # Tokenize the formatted profile\n",
    "        tokenized = tokenizer(formatted_profile, truncation=True, max_length=512)\n",
    "        \n",
    "        processed_data.append({\n",
    "            \"text\": formatted_profile,\n",
    "            \"input_ids\": tokenized[\"input_ids\"],\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "            \"original_data\": profile  # keeping original data for reference\n",
    "        })\n",
    "    \n",
    "    # Save processed data\n",
    "    output_file = os.path.join(processed_dir, f'processed_{dataset_type}_dataset.json')\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(processed_data, f, indent=2)\n",
    "    \n",
    "    print(f\"Processed {len(processed_data)} profiles from {dataset_type} dataset\")\n",
    "    print(f\"Saved to: {output_file}\")\n",
    "    \n",
    "    # Return first example for inspection\n",
    "    return processed_data[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing original dataset...\n",
      "Processed 250 profiles from original dataset\n",
      "Saved to: /Users/cindylinsf/Documents/CCI/THESIS/Msc_Thesis_Project_Files/data/processed/llama_training/processed_original_dataset.json\n",
      "\n",
      "Processing augmented dataset...\n",
      "Processed 250 profiles from augmented dataset\n",
      "Saved to: /Users/cindylinsf/Documents/CCI/THESIS/Msc_Thesis_Project_Files/data/processed/llama_training/processed_augmented_dataset.json\n",
      "\n",
      "Example of processed profile (original dataset):\n",
      "### Instruction:\n",
      "Generate a detailed sperm donor profile based on these characteristics:\n",
      "Height: 5'10 (178cm)\n",
      "Weight: 162 lbs (73kg)\n",
      "Eye Color: Black\n",
      "Hair: Dark Brown\n",
      "Education: Master in Architecture\n",
      "Ethnic Background: East Indian\n",
      "\n",
      "### Response:\n",
      "Quadrilingual Architect. loves his career as an architect. Heâ€™s had a lifelong creative streak and drawing has always been a favorite pastime, from doodling to designing buildings! This smart cookie has an M.S. in architecture (3.7 GPA) and can even speak four languages fluently. To stay in shape, he enjoys tennis, yoga, and weightlifting. He also enjoys quality time in the great outdoors, such as hiking or strolling through parks. With doe eyes and an empathetic personality, heâ€™s as caring as he is handsome!\n",
      "\n",
      "### End\n"
     ]
    }
   ],
   "source": [
    "# Process both datasets\n",
    "try:\n",
    "    # Process original dataset\n",
    "    print(\"\\nProcessing original dataset...\")\n",
    "    original_example = load_and_process_dataset(original_dataset_path, 'original')\n",
    "    \n",
    "    print(\"\\nProcessing augmented dataset...\")\n",
    "    augmented_example = load_and_process_dataset(augmented_dataset_path, 'augmented')\n",
    "    \n",
    "    # Show example of processed profile\n",
    "    print(\"\\nExample of processed profile (original dataset):\")\n",
    "    print(original_example['text'])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error processing data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking original dataset:\n",
      "Total profiles in dataset: 250\n",
      "\n",
      "Keys in each profile:\n",
      "['text', 'input_ids', 'attention_mask', 'original_data']\n",
      "\n",
      "First few tokens decoded:\n",
      "<s> ### Instruction:\n",
      "Generate a detailed\n",
      "\n",
      "All profiles have correct formatting: True\n"
     ]
    }
   ],
   "source": [
    "# Verifying the output\n",
    "\n",
    "import json\n",
    "\n",
    "def inspect_processed_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"Total profiles in dataset: {len(data)}\")\n",
    "    \n",
    "    # Look at first profile\n",
    "    first_profile = data[0]\n",
    "    print(\"\\nKeys in each profile:\")\n",
    "    print(list(first_profile.keys()))\n",
    "    \n",
    "    # Decode a few tokens to show what they represent\n",
    "    print(\"\\nFirst few tokens decoded:\")\n",
    "    decoded = tokenizer.decode(first_profile['input_ids'][:10])\n",
    "    print(decoded)\n",
    "    \n",
    "    # Verify format consistency\n",
    "    format_check = all(\"### Instruction:\" in item['text'] and \n",
    "                      \"### Response:\" in item['text'] and \n",
    "                      \"### End\" in item['text'] \n",
    "                      for item in data)\n",
    "    print(f\"\\nAll profiles have correct formatting: {format_check}\")\n",
    "\n",
    "# Check the processed original dataset\n",
    "print(\"Checking original dataset:\")\n",
    "inspect_processed_data(\"/Users/cindylinsf/Documents/CCI/THESIS/Msc_Thesis_Project_Files/data/processed/llama_training/processed_original_dataset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [07:50<00:00, 235.21s/it]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:21<00:00, 10.83s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "Model and training configuration completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cindylinsf/anaconda3/envs/llama_thesis/lib/python3.9/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, TrainingArguments\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    ")\n",
    "\n",
    "# Load the base model with quantization for memory efficiency\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    torch_dtype=torch.float16,  # Use half precision\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"Base model loaded successfully\")\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                # Rank - number of LoRA pairs\n",
    "    lora_alpha=32,       # Alpha parameter for LoRA scaling\n",
    "    target_modules=[\"q_proj\", \"v_proj\"], # Which modules to apply LoRA to\n",
    "    lora_dropout=0.05,   # Dropout probability for LoRA layers\n",
    "    bias=\"none\",         # We don't train bias parameters\n",
    "    task_type=TaskType.CAUSAL_LM # We're doing causal language modeling\n",
    ")\n",
    "\n",
    "# Prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/Users/cindylinsf/Documents/CCI/THESIS/Msc_Thesis_Project_Files/outputs/llama_fine_tuned\",\n",
    "    num_train_epochs=3,           # Number of training epochs\n",
    "    per_device_train_batch_size=4, # Batch size per device during training\n",
    "    gradient_accumulation_steps=4, # Number of updates steps to accumulate before performing a backward/update pass\n",
    "    learning_rate=2e-4,          # Initial learning rate\n",
    "    warmup_steps=100,            # Number of warmup steps for learning rate scheduler\n",
    "    logging_steps=10,            # Log every X updates steps\n",
    "    save_strategy=\"epoch\",       # Save the model every epoch\n",
    "    evaluation_strategy=\"epoch\", # Evaluate the model every epoch\n",
    "    report_to=\"none\"            # Disable wandb logging\n",
    ")\n",
    "\n",
    "print(\"Model and training configuration completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:14<00:00, 37.14s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model loaded successfully\n",
      "trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.1243\n",
      "Model and training configuration completed\n",
      "\n",
      "Training Configuration Summary:\n",
      "Batch Size: 2\n",
      "Gradient Accumulation Steps: 8\n",
      "Effective Batch Size: 16\n",
      "Number of Epochs: 3\n",
      "Learning Rate: 0.0002\n"
     ]
    }
   ],
   "source": [
    "## Second iteration\n",
    "\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, TrainingArguments\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    ")\n",
    "\n",
    "# Check if MPS is available\n",
    "mps_available = torch.backends.mps.is_available()\n",
    "device = torch.device(\"mps\" if mps_available else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the base model\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    use_cache=False  # Important for training\n",
    ")\n",
    "\n",
    "print(\"Base model loaded successfully\")\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "# Prepare model for training\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()  # This will show us what's being trained\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/Users/cindylinsf/Documents/CCI/THESIS/Msc_Thesis_Project_Files/outputs/llama_fine_tuned\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,  # Reduced batch size\n",
    "    gradient_accumulation_steps=8,  # Increased gradient accumulation\n",
    "    learning_rate=2e-4,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"no\",      # Changed from \"epoch\" to \"no\" to be more efficient and focus purely on training\n",
    "    save_total_limit=3,           # Keep only the last 3 checkpoints\n",
    "    fp16=True,                    # Enable mixed precision training\n",
    "    optim=\"adamw_torch\",          # Use PyTorch's AdamW optimizer\n",
    ")\n",
    "\n",
    "print(\"Model and training configuration completed\")\n",
    "\n",
    "# Print some configuration details for documentation\n",
    "print(\"\\nTraining Configuration Summary:\")\n",
    "print(f\"Batch Size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"Gradient Accumulation Steps: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Effective Batch Size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Number of Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"Learning Rate: {training_args.learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results reflected the advantage of using LoRA, making training efficient while maintaining model capabilities\n",
    "\n",
    "### Model Parameters:\n",
    "\n",
    "- Trainable params: 8,388,608 (about 8M)\n",
    "- Total params: 6,746,804,224 (about 6.7B)\n",
    "- Only training 0.1243% of parameters\n",
    "\n",
    "### Training Configuration:\n",
    "\n",
    "- Batch Size: 2 (per device)\n",
    "- Gradient Accumulation: 8 steps\n",
    "- Effective Batch Size: 16 (2 * 8)\n",
    "- 3 epochs\n",
    "- Learning Rate: 0.0002 (2e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training\n",
    "- The following section can be ignored\n",
    "- I kept running into issues with compatibility so I moved the rest to a Google Colab notebook, which you will find in the `models` folder of the project file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from datasets import Dataset\n",
    "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG - Initial PyTorch setup:\n",
      "MPS available: True\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Added this to force PyTorch to not use MPS since I kept running into issues with the actual training step\n",
    "\n",
    "import torch\n",
    "import os\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'\n",
    "\n",
    "# Verify environment\n",
    "print(\"DEBUG - Initial PyTorch setup:\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "print(f\"Using device: {torch.device('cpu')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. First, modify the prepare_dataset function to handle padding properly\n",
    "def prepare_dataset(file_path):\n",
    "    print(f\"Loading data from {file_path}\")\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(\"Converting data format...\")\n",
    "    try:\n",
    "        # Convert to format expected by trainer\n",
    "        formatted_data = []\n",
    "        for item in data:\n",
    "            # Tokenize with padding and truncation\n",
    "            tokenized = tokenizer(\n",
    "                item['text'],\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=512,  # Adjust this value based on your data\n",
    "                return_tensors=None  # Return lists instead of tensors\n",
    "            )\n",
    "            formatted_data.append({\n",
    "                'input_ids': tokenized['input_ids'],\n",
    "                'attention_mask': tokenized['attention_mask'],\n",
    "                'labels': tokenized['input_ids'].copy()  # For causal language modeling\n",
    "            })\n",
    "        \n",
    "        return Dataset.from_list(formatted_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in prepare_dataset: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original dataset...\n",
      "Loading data from /Users/cindylinsf/Documents/CCI/THESIS/Msc_Thesis_Project_Files/data/processed/llama_training/processed_original_dataset.json\n",
      "Converting data format...\n",
      "Original dataset size: 250\n"
     ]
    }
   ],
   "source": [
    "# 2. Load and prepare the dataset\n",
    "print(\"Loading original dataset...\")\n",
    "try:\n",
    "    train_dataset_original = prepare_dataset(\"/Users/cindylinsf/Documents/CCI/THESIS/Msc_Thesis_Project_Files/data/processed/llama_training/processed_original_dataset.json\")\n",
    "    print(f\"Original dataset size: {len(train_dataset_original)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 225\n",
      "Evaluation set size: 25\n"
     ]
    }
   ],
   "source": [
    "# 3. Split dataset into train/eval\n",
    "train_test_split = train_dataset_original.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Evaluation set size: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized on CPU\n"
     ]
    }
   ],
   "source": [
    "# 4. Initialize model on device\n",
    "device = torch.device(\"cpu\")\n",
    "# Use to_empty() instead of to() for meta tensors\n",
    "model = model.to_empty(device=device)\n",
    "print(\"Model initialized on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DEBUG - Verifying trainer setup:\n",
      "Trainer device: cpu\n",
      "Data collator device: pt\n",
      "Model device map: {'model.embed_tokens': 'mps', 'model.layers.0': 'mps', 'model.layers.1': 'mps', 'model.layers.2': 'mps', 'model.layers.3': 'mps', 'model.layers.4': 'mps', 'model.layers.5': 'mps', 'model.layers.6': 'mps', 'model.layers.7': 'disk', 'model.layers.8': 'disk', 'model.layers.9': 'disk', 'model.layers.10': 'disk', 'model.layers.11': 'disk', 'model.layers.12': 'disk', 'model.layers.13': 'disk', 'model.layers.14': 'disk', 'model.layers.15': 'disk', 'model.layers.16': 'disk', 'model.layers.17': 'disk', 'model.layers.18': 'disk', 'model.layers.19': 'disk', 'model.layers.20': 'disk', 'model.layers.21': 'disk', 'model.layers.22': 'disk', 'model.layers.23': 'disk', 'model.layers.24': 'disk', 'model.layers.25': 'disk', 'model.layers.26': 'disk', 'model.layers.27': 'disk', 'model.layers.28': 'disk', 'model.layers.29': 'disk', 'model.layers.30': 'disk', 'model.layers.31': 'disk', 'model.norm': 'disk', 'model.rotary_emb': 'disk', 'lm_head': 'disk'}\n"
     ]
    }
   ],
   "source": [
    "# 5. Update training arguments with stricter CPU settings\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/Users/cindylinsf/Documents/CCI/THESIS/Msc_Thesis_Project_Files/outputs/llama_fine_tuned\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=10,\n",
    "    \n",
    "    # Matching save and evaluation strategies\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,               \n",
    "    eval_strategy=\"steps\",      \n",
    "    eval_steps=50,              \n",
    "    \n",
    "    save_total_limit=3,         \n",
    "    optim=\"adamw_torch\",\n",
    "    \n",
    "    # Stricter CPU settings\n",
    "    use_cpu=True,\n",
    "    no_cuda=True,\n",
    "    use_mps_device=False,\n",
    "    \n",
    "    # Checkpointing settings\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # Data processing settings\n",
    "    remove_unused_columns=False,\n",
    "    max_grad_norm=0.3,\n",
    "    \n",
    "    # Additional settings\n",
    "    evaluation_strategy=\"steps\",\n",
    "    dataloader_pin_memory=False,\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    # Add these device-specific settings\n",
    "    hub_token=None,\n",
    "    torch_compile=False,\n",
    "    use_ipex=False,\n",
    ")\n",
    "\n",
    "# After creating the trainer, add this verification\n",
    "print(\"\\nDEBUG - Verifying trainer setup:\")\n",
    "print(f\"Trainer device: {trainer.args.device}\")\n",
    "print(f\"Data collator device: {data_collator.return_tensors}\")\n",
    "print(f\"Model device map: {model.hf_device_map if hasattr(model, 'hf_device_map') else 'Not using device map'}\")\n",
    "\n",
    "# Before starting training, force model to CPU again\n",
    "model = model.to('cpu')\n",
    "for param in model.parameters():\n",
    "    if param.device.type != 'cpu':\n",
    "        param.data = param.data.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Initialize the data collator with explicit device setting\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=8,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Make sure all model parameters are on CPU\n",
    "for param in model.parameters():\n",
    "    param.data = param.data.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/42 [04:07<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DEBUG - Before trainer initialization:\n",
      "Device settings from training args:\n",
      "use_cpu: True\n",
      "no_cuda: True\n",
      "use_mps_device: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 8. Initialize the trainer with explicit device handling and debug prints\n",
    "print(\"\\nDEBUG - Before trainer initialization:\")\n",
    "print(f\"Device settings from training args:\")\n",
    "print(f\"use_cpu: {training_args.use_cpu}\")\n",
    "print(f\"no_cuda: {training_args.no_cuda}\")\n",
    "print(f\"use_mps_device: {training_args.use_mps_device}\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forcing all model components to CPU...\n",
      "\n",
      "Device check after forcing CPU:\n",
      "Model device: cpu\n",
      "\n",
      "Verification complete. Ready to start training.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Forcing all model components to CPU...\")\n",
    "\n",
    "# 1. Force model and all parameters to CPU explicitly\n",
    "model = model.to('cpu')\n",
    "model.device = torch.device('cpu')  # Explicitly set device attribute\n",
    "\n",
    "# 2. Ensure all model parameters are on CPU\n",
    "for param in model.parameters():\n",
    "    if hasattr(param, 'data'):\n",
    "        param.data = param.data.to('cpu')\n",
    "    if hasattr(param, 'grad') and param.grad is not None:\n",
    "        param.grad.data = param.grad.data.to('cpu')\n",
    "\n",
    "# 3. Verify device placement\n",
    "print(\"\\nDevice check after forcing CPU:\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.device.type != 'cpu':\n",
    "        print(f\"Warning: {name} is on {param.device}\")\n",
    "\n",
    "print(\"\\nVerification complete. Ready to start training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinitializing model with explicit CPU configuration...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReinitializing model with explicit CPU configuration...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 1. First, reset PEFT configuration\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m lora_config \u001b[38;5;241m=\u001b[39m \u001b[43mLoraConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mq_proj\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mv_proj\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTaskType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCAUSAL_LM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43minference_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Explicitly set device\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 2. Reinitialize base model with explicit CPU config\u001b[39;00m\n\u001b[1;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m LlamaForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-2-7b-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32,  \u001b[38;5;66;03m# Use float32 instead of float16 for CPU\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# Disable device mapping\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     load_in_8bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# Disable 8-bit loading\u001b[39;00m\n\u001b[1;32m     21\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'device'"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Reinitializing model with explicit CPU configuration...\")\n",
    "\n",
    "# 1. First, reset PEFT configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    device='cpu'  # Explicitly set device\n",
    ")\n",
    "\n",
    "# 2. Reinitialize base model with explicit CPU config\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    torch_dtype=torch.float32,  # Use float32 instead of float16 for CPU\n",
    "    device_map=None,  # Disable device mapping\n",
    "    load_in_8bit=False,  # Disable 8-bit loading\n",
    ")\n",
    "\n",
    "# 3. Force model to CPU before PEFT\n",
    "model = model.to('cpu')\n",
    "\n",
    "# 4. Create PEFT model with additional safety checks\n",
    "def create_safe_peft_model(model, config):\n",
    "    # Ensure model is on CPU\n",
    "    model = model.to('cpu')\n",
    "    \n",
    "    # Create PEFT model\n",
    "    peft_model = get_peft_model(model, config)\n",
    "    \n",
    "    # Force PEFT model to CPU\n",
    "    peft_model = peft_model.to('cpu')\n",
    "    \n",
    "    # Verify all components are on CPU\n",
    "    for name, module in peft_model.named_modules():\n",
    "        if hasattr(module, 'to'):\n",
    "            module.to('cpu')\n",
    "    \n",
    "    return peft_model\n",
    "\n",
    "# 5. Create PEFT model with safety checks\n",
    "model = create_safe_peft_model(model, lora_config)\n",
    "\n",
    "# 6. Verify model state\n",
    "print(\"\\nVerifying model configuration:\")\n",
    "print(f\"Base model device: {next(model.parameters()).device}\")\n",
    "print(f\"Model class: {type(model)}\")\n",
    "\n",
    "# 7. Print some PEFT-specific information\n",
    "if hasattr(model, 'active_adapter'):\n",
    "    print(f\"Active adapter: {model.active_adapter}\")\n",
    "if hasattr(model, 'peft_config'):\n",
    "    print(\"PEFT config:\", model.peft_config)\n",
    "\n",
    "# 8. Reinitialize trainer with updated model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"\\nModel and trainer reinitialization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DEBUG - Starting enhanced training process...\n",
      "\n",
      "DEBUG - Testing single batch before training:\n",
      "Batch devices:\n",
      "input_ids: device=cpu, dtype=torch.int64\n",
      "attention_mask: device=cpu, dtype=torch.int64\n",
      "labels: device=cpu, dtype=torch.int64\n",
      "\n",
      "DEBUG - Attempting custom forward pass...\n",
      "\n",
      "DEBUG - Input devices after processing:\n",
      "input_ids: device=cpu, shape=torch.Size([1, 512])\n",
      "attention_mask: device=cpu, shape=torch.Size([1, 512])\n",
      "labels: device=cpu, shape=torch.Size([1, 512])\n",
      "\n",
      "Error in forward pass: Placeholder storage has not been allocated on MPS device!\n",
      "Model device: cpu\n",
      "\n",
      "DEBUG - Error occurred:\n",
      "Error type: <class 'RuntimeError'>\n",
      "Error message: Placeholder storage has not been allocated on MPS device!\n",
      "\n",
      "Model state:\n",
      "Model class: <class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "Model device: cpu\n",
      "Current state saved successfully.\n",
      "\n",
      "Training process completed or interrupted\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Training execution with custom forward pass\n",
    "print(\"\\nDEBUG - Starting enhanced training process...\")\n",
    "\n",
    "# First, ensure everything is on CPU and MPS is disabled\n",
    "if hasattr(torch.mps, 'empty_cache'):\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "# Verify and force CPU for all model components\n",
    "def ensure_cpu_tensors(model):\n",
    "    model = model.to('cpu')\n",
    "    if hasattr(model, 'device'):\n",
    "        model.device = torch.device('cpu')\n",
    "    \n",
    "    # Force all buffers to CPU\n",
    "    for buffer in model.buffers():\n",
    "        if buffer is not None:\n",
    "            buffer.data = buffer.data.to('cpu')\n",
    "    \n",
    "    # Force all parameters to CPU\n",
    "    for param in model.parameters():\n",
    "        if param is not None:\n",
    "            if hasattr(param, 'data'):\n",
    "                param.data = param.data.to('cpu')\n",
    "            if hasattr(param, 'grad') and param.grad is not None:\n",
    "                param.grad.data = param.grad.data.to('cpu')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Apply CPU enforcement\n",
    "trainer.model = ensure_cpu_tensors(trainer.model)\n",
    "\n",
    "# Custom forward function\n",
    "def safe_forward(model, batch):\n",
    "    # Ensure inputs are on CPU and properly formatted\n",
    "    inputs = {\n",
    "        k: v.to('cpu') if torch.is_tensor(v) else v\n",
    "        for k, v in batch.items()\n",
    "    }\n",
    "    \n",
    "    # Remove any None values\n",
    "    inputs = {k: v for k, v in inputs.items() if v is not None}\n",
    "    \n",
    "    print(\"\\nDEBUG - Input devices after processing:\")\n",
    "    for k, v in inputs.items():\n",
    "        if torch.is_tensor(v):\n",
    "            print(f\"{k}: device={v.device}, shape={v.shape}\")\n",
    "    \n",
    "    # Perform forward pass with error checking\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        return outputs\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in forward pass: {str(e)}\")\n",
    "        print(f\"Model device: {next(model.parameters()).device}\")\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    print(\"\\nDEBUG - Testing single batch before training:\")\n",
    "    sample_dataloader = trainer.get_train_dataloader()\n",
    "    sample_batch = next(iter(sample_dataloader))\n",
    "    \n",
    "    print(\"Batch devices:\")\n",
    "    for key, value in sample_batch.items():\n",
    "        if hasattr(value, 'device'):\n",
    "            print(f\"{key}: device={value.device}, dtype={value.dtype}\")\n",
    "    \n",
    "    print(\"\\nDEBUG - Attempting custom forward pass...\")\n",
    "    outputs = safe_forward(trainer.model, sample_batch)\n",
    "    print(\"Single forward pass successful!\")\n",
    "    \n",
    "    # If forward pass succeeds, proceed with training\n",
    "    print(\"\\nStarting full training...\")\n",
    "    trainer.train(resume_from_checkpoint=False)\n",
    "    \n",
    "    # Save the final model\n",
    "    output_dir = \"/Users/cindylinsf/Documents/CCI/THESIS/Msc_Thesis_Project_Files/outputs/llama_fine_tuned/final_model\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    trainer.save_model(output_dir)\n",
    "    print(f\"\\nModel saved to {output_dir}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"\\nDEBUG - Error occurred:\")\n",
    "    print(f\"Error type: {type(e)}\")\n",
    "    print(f\"Error message: {str(e)}\")\n",
    "    \n",
    "    # Additional debugging information\n",
    "    print(\"\\nModel state:\")\n",
    "    print(f\"Model class: {type(trainer.model)}\")\n",
    "    print(f\"Model device: {next(trainer.model.parameters()).device}\")\n",
    "    \n",
    "    # Try to save current state\n",
    "    try:\n",
    "        save_dir = \"/Users/cindylinsf/Documents/CCI/THESIS/Msc_Thesis_Project_Files/outputs/llama_fine_tuned/error_state\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        trainer.save_model(save_dir)\n",
    "        print(\"Current state saved successfully.\")\n",
    "    except Exception as save_error:\n",
    "        print(f\"Could not save current state: {save_error}\")\n",
    "\n",
    "print(\"\\nTraining process completed or interrupted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
