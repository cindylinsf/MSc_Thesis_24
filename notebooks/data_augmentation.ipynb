{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataset B\n",
    "- Balanced distributions of ethnicities reflected in the original cleaned dataset\n",
    "- Disgarded \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import hashlib\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DonorDataAugmenter:\n",
    "    def __init__(self, input_path: str, output_dir: str):\n",
    "        self.input_path = input_path\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.original_data = None\n",
    "        self.augmented_data = None\n",
    "        self.transformation_summary = defaultdict(dict)\n",
    "    \n",
    "    def standardize_ethnicity(self, ethnicity: str) -> str:\n",
    "        \"\"\"Standardize ethnicity names and handle combined ethnicities.\"\"\"\n",
    "        if pd.isna(ethnicity):\n",
    "            return \"Unknown\"\n",
    "        \n",
    "        # Handle the special case of mother/father ethnic origin\n",
    "        if ethnicity.startswith(\"Mother:\"):\n",
    "            return \"Mixed or Multi Ethnic\"  # Simplify complex parent combinations\n",
    "            \n",
    "        # Basic standardization map\n",
    "        standard_names = {\n",
    "            'Caucasian/White': 'Caucasian',\n",
    "            'Latino/Hispanic': 'Hispanic or Latino',\n",
    "            'Latino': 'Hispanic or Latino',\n",
    "            'African American/Black': 'Black or African American',\n",
    "            'Black': 'Black or African American',\n",
    "            'Multi/Mixed': 'Mixed or Multi Ethnic',\n",
    "            'Multi': 'Mixed or Multi Ethnic',\n",
    "            'Middle Eastern/African': 'Middle Eastern or Arabic',\n",
    "            'Pacific Islander': 'Native Hawaiian or Other Pacific Islander',\n",
    "            'American Indian': 'American Indian or Alaska Native'\n",
    "        }\n",
    "        \n",
    "        # If it's a combined ethnicity, split, standardize each part, and sort\n",
    "        if ',' in ethnicity:\n",
    "            parts = [part.strip() for part in ethnicity.split(',')]\n",
    "            standardized_parts = []\n",
    "            for part in parts:\n",
    "                # Standardize each individual ethnicity\n",
    "                std_part = standard_names.get(part, part)\n",
    "                standardized_parts.append(std_part)\n",
    "            # Sort to ensure consistent ordering\n",
    "            return ', '.join(sorted(set(standardized_parts)))  # Use set to remove duplicates\n",
    "        \n",
    "        return standard_names.get(ethnicity, ethnicity)\n",
    "\n",
    "    def preprocess_ethnicity_data(self) -> None:\n",
    "        \"\"\"Preprocess ethnicity data to combine similar categories.\"\"\"\n",
    "        if 'ethnic_background' not in self.df.columns:\n",
    "            return\n",
    "            \n",
    "        # First standardize all ethnicities\n",
    "        self.df['ethnic_background'] = self.df['ethnic_background'].apply(self.standardize_ethnicity)\n",
    "        \n",
    "        # Group similar multi-ethnic combinations\n",
    "        def simplify_multi_ethnic(ethnicity: str) -> str:\n",
    "            if pd.isna(ethnicity) or ethnicity == \"Unknown\":\n",
    "                return ethnicity\n",
    "                \n",
    "            parts = set(part.strip() for part in ethnicity.split(','))\n",
    "            if len(parts) > 2:  # If more than two ethnicities\n",
    "                return \"Mixed or Multi Ethnic\"\n",
    "                \n",
    "            return ethnicity\n",
    "        \n",
    "        self.df['ethnic_background'] = self.df['ethnic_background'].apply(simplify_multi_ethnic)\n",
    "        \n",
    "        # Count frequency of each category\n",
    "        ethnicity_counts = self.df['ethnic_background'].value_counts()\n",
    "        \n",
    "        # Combine rare categories (those with only 1-2 examples) into broader categories\n",
    "        rare_threshold = 3\n",
    "        for idx, count in ethnicity_counts.items():\n",
    "            if count < rare_threshold and ',' in idx:\n",
    "                self.df.loc[self.df['ethnic_background'] == idx, 'ethnic_background'] = 'Mixed or Multi Ethnic'\n",
    "        \n",
    "    def load_data(self) -> None:\n",
    "        \"\"\"Load and preprocess the original JSON dataset.\"\"\"\n",
    "        with open(self.input_path, 'r') as f:\n",
    "            self.original_data = json.load(f)\n",
    "        \n",
    "        # Convert to DataFrame for easier processing\n",
    "        self.df = pd.DataFrame(self.original_data)\n",
    "        \n",
    "        # Preprocess ethnicity data\n",
    "        self.preprocess_ethnicity_data()\n",
    "        \n",
    "        # Store original distribution\n",
    "        self.transformation_summary['original_distribution'] = self.get_distribution_stats(self.df)\n",
    "\n",
    "    def get_distribution_stats(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Calculate distribution statistics for the dataset.\"\"\"\n",
    "        ethnic_dist = df['ethnic_background'].value_counts().to_dict()\n",
    "        \n",
    "        education_by_ethnicity = {}\n",
    "        for ethnicity in df['ethnic_background'].unique():\n",
    "            if pd.isna(ethnicity) or ethnicity == \"Unknown\":\n",
    "                continue\n",
    "            education_counts = df[df['ethnic_background'] == ethnicity]['education_level'].value_counts()\n",
    "            education_by_ethnicity[ethnicity] = education_counts.to_dict()\n",
    "        \n",
    "        stats = {\n",
    "            'ethnic_distribution': ethnic_dist,\n",
    "            'education_by_ethnicity': education_by_ethnicity,\n",
    "            'total_profiles': len(df),\n",
    "            'null_values': df.isnull().sum().to_dict()\n",
    "        }\n",
    "        return stats\n",
    "\n",
    "    def get_profile_hash(self, profile: Dict) -> str:\n",
    "        \"\"\"Create a more robust hash of profile attributes.\"\"\"\n",
    "        key_attrs = [\n",
    "            'donor_description',\n",
    "            'height',\n",
    "            'weight',\n",
    "            'education_level',\n",
    "            'education_field',\n",
    "            'eye_color',\n",
    "            'hair_color',\n",
    "            'ethnic_background'\n",
    "        ]\n",
    "        \n",
    "        # Include attribute names and timestamp in hash\n",
    "        hash_parts = []\n",
    "        for attr in key_attrs:\n",
    "            hash_parts.append(f\"{attr}:{str(profile.get(attr, ''))}\")\n",
    "        \n",
    "        # Add timestamp to make hash more unique\n",
    "        hash_parts.append(f\"timestamp:{time.time()}\")\n",
    "        \n",
    "        hash_str = '|'.join(hash_parts)\n",
    "        return hashlib.md5(hash_str.encode()).hexdigest()\n",
    "\n",
    "    def modify_text(self, text: str) -> str:\n",
    "        \"\"\"Make more substantial modifications to text while maintaining meaning.\"\"\"\n",
    "        if not text:\n",
    "            return text\n",
    "            \n",
    "        # Expanded replacements dictionary with more variations\n",
    "        replacements = {\n",
    "            'enjoys': ['likes', 'loves', 'is passionate about', 'takes pleasure in', 'has a fondness for', 'delights in'],\n",
    "            'currently': ['presently', 'now', 'at present', 'at the moment', 'these days', 'as of now'],\n",
    "            'studying': ['pursuing', 'working on', 'learning', 'focused on', 'specializing in', 'dedicated to'],\n",
    "            'passionate': ['enthusiastic', 'dedicated', 'committed', 'devoted', 'driven', 'motivated'],\n",
    "            'loves': ['enjoys', 'is passionate about', 'is enthusiastic about', 'adores', 'cherishes', 'treasures'],\n",
    "            'working': ['employed', 'pursuing a career', 'engaged', 'involved', 'active', 'focused'],\n",
    "            'interested': ['passionate', 'keen', 'focused', 'devoted', 'engaged', 'invested'],\n",
    "            'like': ['enjoy', 'appreciate', 'value', 'favor', 'prefer', 'gravitate towards'],\n",
    "            'very': ['quite', 'particularly', 'especially', 'notably', 'remarkably', 'exceptionally'],\n",
    "            'good': ['great', 'excellent', 'exceptional', 'outstanding', 'impressive', 'remarkable']\n",
    "        }\n",
    "        \n",
    "        modified_text = text\n",
    "        # Apply multiple replacements with randomization\n",
    "        for _ in range(np.random.randint(2, 5)):  # Variable number of replacements\n",
    "            for word, alternatives in replacements.items():\n",
    "                if word in modified_text.lower():\n",
    "                    replacement = np.random.choice(alternatives)\n",
    "                    modified_text = modified_text.replace(word, replacement)\n",
    "        \n",
    "        # Add sentence modifiers with more variety\n",
    "        modifiers = [\n",
    "            \"In general, \",\n",
    "            \"For the most part, \",\n",
    "            \"Primarily, \",\n",
    "            \"Characteristically, \",\n",
    "            \"Typically, \",\n",
    "            \"As a person, \",\n",
    "            \"Overall, \",\n",
    "            \"By nature, \"\n",
    "        ]\n",
    "        \n",
    "        if np.random.random() < 0.4:  # 40% chance to add modifier\n",
    "            modified_text = np.random.choice(modifiers) + modified_text.lower()\n",
    "        \n",
    "        return modified_text\n",
    "\n",
    "    def create_synthetic_profile(self, ethnic_group: pd.DataFrame, used_hashes: set, max_attempts: int = 100) -> Dict:\n",
    "        \"\"\"Create a more diverse synthetic profile with enhanced variation.\"\"\"\n",
    "        ethnic_profiles = ethnic_group.to_dict('records')\n",
    "        \n",
    "        for attempt in range(max_attempts):\n",
    "            # Select more templates for mixing\n",
    "            n_templates = min(5, len(ethnic_profiles))\n",
    "            templates = np.random.choice(ethnic_profiles, size=n_templates, replace=False)\n",
    "            \n",
    "            # Start with base template\n",
    "            synthetic_profile = copy.deepcopy(templates[0])\n",
    "            synthetic_profile['source'] = 'synthetic'\n",
    "            \n",
    "            # Mix features more aggressively\n",
    "            if len(templates) > 1:\n",
    "                for attr in ['height', 'weight', 'eye_color', 'hair_color', 'education_level', 'education_field']:\n",
    "                    if np.random.random() < 0.6:  # 60% chance to take from another template\n",
    "                        template = np.random.choice(templates[1:])\n",
    "                        if template.get(attr):\n",
    "                            synthetic_profile[attr] = template[attr]\n",
    "            \n",
    "            # Add variations to numerical values\n",
    "            if synthetic_profile.get('height'):\n",
    "                try:\n",
    "                    height_val = float(synthetic_profile['height'].replace(\"'\", \".\"))\n",
    "                    height_val += np.random.uniform(-0.3, 0.3)  # Larger random adjustment\n",
    "                    synthetic_profile['height'] = f\"{height_val:.1f}'\"\n",
    "                except (ValueError, AttributeError):\n",
    "                    pass\n",
    "                \n",
    "            if synthetic_profile.get('weight'):\n",
    "                try:\n",
    "                    weight_val = float(synthetic_profile['weight'])\n",
    "                    weight_val += np.random.randint(-8, 9)  # Larger random adjustment\n",
    "                    synthetic_profile['weight'] = str(int(weight_val))\n",
    "                except (ValueError, TypeError):\n",
    "                    pass\n",
    "            \n",
    "            # Enhanced description modification\n",
    "            if synthetic_profile.get('donor_description'):\n",
    "                modified_description = synthetic_profile['donor_description']\n",
    "                for _ in range(np.random.randint(2, 5)):  # Variable number of modifications\n",
    "                    modified_description = self.modify_text(modified_description)\n",
    "                synthetic_profile['donor_description'] = modified_description\n",
    "            \n",
    "            # Add timestamp to ensure uniqueness\n",
    "            synthetic_profile['generation_timestamp'] = time.time()\n",
    "            \n",
    "            profile_hash = self.get_profile_hash(synthetic_profile)\n",
    "            if profile_hash not in used_hashes:\n",
    "                used_hashes.add(profile_hash)\n",
    "                return synthetic_profile\n",
    "        \n",
    "        # If we still couldn't create a unique profile\n",
    "        base_profile = copy.deepcopy(np.random.choice(ethnic_profiles))\n",
    "        base_profile['source'] = 'synthetic'\n",
    "        timestamp = int(time.time() * 1000000)  # microsecond timestamp\n",
    "        base_profile['donor_description'] = f\"{base_profile.get('donor_description', '')} [Variant {timestamp}]\"\n",
    "        base_profile['generation_timestamp'] = time.time()\n",
    "        \n",
    "        profile_hash = self.get_profile_hash(base_profile)\n",
    "        used_hashes.add(profile_hash)\n",
    "        \n",
    "        return base_profile\n",
    "\n",
    "    def create_balanced_dataset(self) -> None:\n",
    "        \"\"\"Create a balanced dataset with equal ethnic representation.\"\"\"\n",
    "        df = self.df.copy()\n",
    "        \n",
    "        # Remove 'Unknown' category and NA values\n",
    "        df = df[df['ethnic_background'].notna()]\n",
    "        df = df[df['ethnic_background'] != 'Unknown']\n",
    "        \n",
    "        # Calculate target number per category\n",
    "        total_target = len(self.original_data)\n",
    "        n_categories = len(df['ethnic_background'].unique())\n",
    "        target_per_category = total_target // n_categories\n",
    "        \n",
    "        balanced_data = []\n",
    "        used_hashes = set()\n",
    "        \n",
    "        # Process each ethnic category\n",
    "        for ethnicity in sorted(df['ethnic_background'].unique()):\n",
    "            ethnic_group = df[df['ethnic_background'] == ethnicity]\n",
    "            print(f\"\\nProcessing ethnicity: {ethnicity}\")\n",
    "            print(f\"Original profiles: {len(ethnic_group)}\")\n",
    "            \n",
    "            if len(ethnic_group) >= target_per_category:\n",
    "                selected = ethnic_group.sample(n=target_per_category, random_state=42)\n",
    "                for _, row in selected.iterrows():\n",
    "                    profile = row.to_dict()\n",
    "                    profile['source'] = 'original'\n",
    "                    profile_hash = self.get_profile_hash(profile)\n",
    "                    used_hashes.add(profile_hash)\n",
    "                    balanced_data.append(profile)\n",
    "            else:\n",
    "                # Include all original profiles\n",
    "                for _, row in ethnic_group.iterrows():\n",
    "                    profile = row.to_dict()\n",
    "                    profile['source'] = 'original'\n",
    "                    profile_hash = self.get_profile_hash(profile)\n",
    "                    used_hashes.add(profile_hash)\n",
    "                    balanced_data.append(profile)\n",
    "                \n",
    "                # Synthesize additional profiles\n",
    "                n_synthetic = target_per_category - len(ethnic_group)\n",
    "                created_synthetic = 0\n",
    "                print(f\"Creating {n_synthetic} synthetic profiles\")\n",
    "                \n",
    "                while created_synthetic < n_synthetic:\n",
    "                    try:\n",
    "                        synthetic_profile = self.create_synthetic_profile(ethnic_group, used_hashes)\n",
    "                        balanced_data.append(synthetic_profile)\n",
    "                        created_synthetic += 1\n",
    "                        if created_synthetic % 5 == 0:  # Progress update every 5 profiles\n",
    "                            print(f\"Created {created_synthetic}/{n_synthetic} synthetic profiles\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Failed to create synthetic profile: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                print(f\"Completed {created_synthetic}/{n_synthetic} synthetic profiles\")\n",
    "        \n",
    "        self.augmented_data = balanced_data\n",
    "        augmented_df = pd.DataFrame(balanced_data)\n",
    "        self.transformation_summary['augmented_distribution'] = self.get_distribution_stats(augmented_df)\n",
    "        print(f\"\\nFinal dataset size: {len(balanced_data)} profiles\")\n",
    "        \n",
    "        # Print distribution comparison\n",
    "        print(\"\\nEthnic distribution comparison:\")\n",
    "        original_dist = pd.Series(self.transformation_summary['original_distribution']['ethnic_distribution'])\n",
    "        augmented_dist = pd.Series(self.transformation_summary['augmented_distribution']['ethnic_distribution'])\n",
    "        comparison = pd.DataFrame({\n",
    "            'Original': original_dist,\n",
    "            'Augmented': augmented_dist\n",
    "        }).fillna(0)\n",
    "        print(comparison)\n",
    "\n",
    "    def save_outputs(self) -> None:\n",
    "        \"\"\"Save all output files.\"\"\"\n",
    "        # Save augmented dataset as JSON\n",
    "        with open(self.output_dir / 'augmented_dataset.json', 'w') as f:\n",
    "            json.dump(self.augmented_data, f, indent=2)\n",
    "        \n",
    "        # Save as CSV\n",
    "        pd.DataFrame(self.augmented_data).to_csv(\n",
    "            self.output_dir / 'augmented_dataset.csv', index=False\n",
    "        )\n",
    "        \n",
    "        # Add transformation metadata\n",
    "        self.transformation_summary['metadata'] = {\n",
    "            'original_file': self.input_path,\n",
    "            'total_original_profiles': len(self.original_data),\n",
    "            'total_augmented_profiles': len(self.augmented_data),\n",
    "            'synthetic_profiles_count': sum(1 for p in self.augmented_data if p['source'] == 'synthetic'),\n",
    "            'original_profiles_count': sum(1 for p in self.augmented_data if p['source'] == 'original'),\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "        \n",
    "        # Save transformation summary\n",
    "        with open(self.output_dir / 'transformation_summary.json', 'w') as f:\n",
    "            json.dump(dict(self.transformation_summary), f, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ethnicity: American Indian or Alaska Native, Caucasian\n",
      "Original profiles: 9\n",
      "Creating 69 synthetic profiles\n",
      "Created 5/69 synthetic profiles\n",
      "Created 10/69 synthetic profiles\n",
      "Created 15/69 synthetic profiles\n",
      "Created 20/69 synthetic profiles\n",
      "Created 25/69 synthetic profiles\n",
      "Created 30/69 synthetic profiles\n",
      "Created 35/69 synthetic profiles\n",
      "Created 40/69 synthetic profiles\n",
      "Created 45/69 synthetic profiles\n",
      "Created 50/69 synthetic profiles\n",
      "Created 55/69 synthetic profiles\n",
      "Created 60/69 synthetic profiles\n",
      "Created 65/69 synthetic profiles\n",
      "Completed 69/69 synthetic profiles\n",
      "\n",
      "Processing ethnicity: Asian\n",
      "Original profiles: 219\n",
      "\n",
      "Processing ethnicity: Asian, Caucasian\n",
      "Original profiles: 19\n",
      "Creating 59 synthetic profiles\n",
      "Created 5/59 synthetic profiles\n",
      "Created 10/59 synthetic profiles\n",
      "Created 15/59 synthetic profiles\n",
      "Created 20/59 synthetic profiles\n",
      "Created 25/59 synthetic profiles\n",
      "Created 30/59 synthetic profiles\n",
      "Created 35/59 synthetic profiles\n",
      "Created 40/59 synthetic profiles\n",
      "Created 45/59 synthetic profiles\n",
      "Created 50/59 synthetic profiles\n",
      "Created 55/59 synthetic profiles\n",
      "Completed 59/59 synthetic profiles\n",
      "\n",
      "Processing ethnicity: Black or African American\n",
      "Original profiles: 48\n",
      "Creating 30 synthetic profiles\n",
      "Created 5/30 synthetic profiles\n",
      "Created 10/30 synthetic profiles\n",
      "Created 15/30 synthetic profiles\n",
      "Created 20/30 synthetic profiles\n",
      "Created 25/30 synthetic profiles\n",
      "Created 30/30 synthetic profiles\n",
      "Completed 30/30 synthetic profiles\n",
      "\n",
      "Processing ethnicity: Black or African American, Caucasian\n",
      "Original profiles: 5\n",
      "Creating 73 synthetic profiles\n",
      "Created 5/73 synthetic profiles\n",
      "Created 10/73 synthetic profiles\n",
      "Created 15/73 synthetic profiles\n",
      "Created 20/73 synthetic profiles\n",
      "Created 25/73 synthetic profiles\n",
      "Created 30/73 synthetic profiles\n",
      "Created 35/73 synthetic profiles\n",
      "Created 40/73 synthetic profiles\n",
      "Created 45/73 synthetic profiles\n",
      "Created 50/73 synthetic profiles\n",
      "Created 55/73 synthetic profiles\n",
      "Created 60/73 synthetic profiles\n",
      "Created 65/73 synthetic profiles\n",
      "Created 70/73 synthetic profiles\n",
      "Completed 73/73 synthetic profiles\n",
      "\n",
      "Processing ethnicity: Caucasian\n",
      "Original profiles: 330\n",
      "\n",
      "Processing ethnicity: Caucasian, Hispanic or Latino\n",
      "Original profiles: 15\n",
      "Creating 63 synthetic profiles\n",
      "Created 5/63 synthetic profiles\n",
      "Created 10/63 synthetic profiles\n",
      "Created 15/63 synthetic profiles\n",
      "Created 20/63 synthetic profiles\n",
      "Created 25/63 synthetic profiles\n",
      "Created 30/63 synthetic profiles\n",
      "Created 35/63 synthetic profiles\n",
      "Created 40/63 synthetic profiles\n",
      "Created 45/63 synthetic profiles\n",
      "Created 50/63 synthetic profiles\n",
      "Created 55/63 synthetic profiles\n",
      "Created 60/63 synthetic profiles\n",
      "Completed 63/63 synthetic profiles\n",
      "\n",
      "Processing ethnicity: Caucasian, Middle Eastern or Arabic\n",
      "Original profiles: 9\n",
      "Creating 69 synthetic profiles\n",
      "Created 5/69 synthetic profiles\n",
      "Created 10/69 synthetic profiles\n",
      "Created 15/69 synthetic profiles\n",
      "Created 20/69 synthetic profiles\n",
      "Created 25/69 synthetic profiles\n",
      "Created 30/69 synthetic profiles\n",
      "Created 35/69 synthetic profiles\n",
      "Created 40/69 synthetic profiles\n",
      "Created 45/69 synthetic profiles\n",
      "Created 50/69 synthetic profiles\n",
      "Created 55/69 synthetic profiles\n",
      "Created 60/69 synthetic profiles\n",
      "Created 65/69 synthetic profiles\n",
      "Completed 69/69 synthetic profiles\n",
      "\n",
      "Processing ethnicity: East Indian\n",
      "Original profiles: 24\n",
      "Creating 54 synthetic profiles\n",
      "Created 5/54 synthetic profiles\n",
      "Created 10/54 synthetic profiles\n",
      "Created 15/54 synthetic profiles\n",
      "Created 20/54 synthetic profiles\n",
      "Created 25/54 synthetic profiles\n",
      "Created 30/54 synthetic profiles\n",
      "Created 35/54 synthetic profiles\n",
      "Created 40/54 synthetic profiles\n",
      "Created 45/54 synthetic profiles\n",
      "Created 50/54 synthetic profiles\n",
      "Completed 54/54 synthetic profiles\n",
      "\n",
      "Processing ethnicity: Hispanic or Latino\n",
      "Original profiles: 93\n",
      "\n",
      "Processing ethnicity: Hispanic or Latino, Mixed or Multi Ethnic\n",
      "Original profiles: 3\n",
      "Creating 75 synthetic profiles\n",
      "Created 5/75 synthetic profiles\n",
      "Created 10/75 synthetic profiles\n",
      "Created 15/75 synthetic profiles\n",
      "Created 20/75 synthetic profiles\n",
      "Created 25/75 synthetic profiles\n",
      "Created 30/75 synthetic profiles\n",
      "Created 35/75 synthetic profiles\n",
      "Created 40/75 synthetic profiles\n",
      "Created 45/75 synthetic profiles\n",
      "Created 50/75 synthetic profiles\n",
      "Created 55/75 synthetic profiles\n",
      "Created 60/75 synthetic profiles\n",
      "Created 65/75 synthetic profiles\n",
      "Created 70/75 synthetic profiles\n",
      "Created 75/75 synthetic profiles\n",
      "Completed 75/75 synthetic profiles\n",
      "\n",
      "Processing ethnicity: Middle Eastern or Arabic\n",
      "Original profiles: 11\n",
      "Creating 67 synthetic profiles\n",
      "Created 5/67 synthetic profiles\n",
      "Created 10/67 synthetic profiles\n",
      "Created 15/67 synthetic profiles\n",
      "Created 20/67 synthetic profiles\n",
      "Created 25/67 synthetic profiles\n",
      "Created 30/67 synthetic profiles\n",
      "Created 35/67 synthetic profiles\n",
      "Created 40/67 synthetic profiles\n",
      "Created 45/67 synthetic profiles\n",
      "Created 50/67 synthetic profiles\n",
      "Created 55/67 synthetic profiles\n",
      "Created 60/67 synthetic profiles\n",
      "Created 65/67 synthetic profiles\n",
      "Completed 67/67 synthetic profiles\n",
      "\n",
      "Processing ethnicity: Mixed or Multi Ethnic\n",
      "Original profiles: 223\n",
      "\n",
      "Final dataset size: 1014 profiles\n",
      "\n",
      "Ethnic distribution comparison:\n",
      "                                             Original  Augmented\n",
      "American Indian or Alaska Native, Caucasian         9       78.0\n",
      "Asian                                             219       78.0\n",
      "Asian, Caucasian                                   19       78.0\n",
      "Black or African American                          48       78.0\n",
      "Black or African American, Caucasian                5       78.0\n",
      "Caucasian                                         330       78.0\n",
      "Caucasian, Hispanic or Latino                      15       78.0\n",
      "Caucasian, Middle Eastern or Arabic                 9       78.0\n",
      "East Indian                                        24       78.0\n",
      "Hispanic or Latino                                 93       78.0\n",
      "Hispanic or Latino, Mixed or Multi Ethnic           3       78.0\n",
      "Middle Eastern or Arabic                           11       78.0\n",
      "Mixed or Multi Ethnic                             223       78.0\n",
      "Unknown                                            11        0.0\n",
      "\n",
      "Data augmentation complete. Files saved to: /Users/cindylinsf/Documents/CCI/THESIS/Msc_Thesis_Project_Files/data/generated/augmented_dataset\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # File paths\n",
    "    input_path = \"/Users/cindylinsf/Documents/CCI/THESIS/Msc_Thesis_Project_Files/data/processed/merged_donor_data.json\"\n",
    "    output_dir = \"/Users/cindylinsf/Documents/CCI/THESIS/Msc_Thesis_Project_Files/data/generated/augmented_dataset\"\n",
    "    \n",
    "    # Create and run augmenter\n",
    "    augmenter = DonorDataAugmenter(input_path, output_dir)\n",
    "    augmenter.load_data()\n",
    "    augmenter.create_balanced_dataset()\n",
    "    augmenter.save_outputs()\n",
    "    \n",
    "    print(\"\\nData augmentation complete. Files saved to:\", output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Quality of the Augmented Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DATA QUALITY REPORT ===\n",
      "\n",
      "EXACT DUPLICATES:\n",
      "Found 3 exact duplicates\n",
      "\n",
      "SIMILAR DESCRIPTIONS:\n",
      "Found 1395 pairs of very similar descriptions\n",
      "\n",
      "Example of similar pair:\n",
      "Text 1: Caring Military Man. This donor’s proudest moment came in the military, when he was inspecting an ai\n",
      "Text 2: Characteristically, in general, caring military man. this donor’s proudest moment came in the milita\n",
      "Similarity: 0.87\n",
      "\n",
      "PHYSICAL ATTRIBUTE ISSUES:\n",
      "Height issues: 0\n",
      "Weight issues: 1\n",
      "BMI issues: 0\n",
      "\n",
      "TEXT QUALITY ISSUES:\n",
      "very_short: 56 instances\n",
      "unusual_punctuation: 1 instances\n",
      "repeated_words: 1 instances\n",
      "very_long: 10 instances\n",
      "\n",
      "MISSING VALUES:\n",
      "Profiles with any missing values: 1014\n",
      "\n",
      "SYNTHETIC VS ORIGINAL:\n",
      "Original profiles: 455\n",
      "Synthetic profiles: 559\n",
      "Synthetic ratio: 0.55\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "\n",
    "class DataQualityAnalyzer:\n",
    "    def __init__(self, file_path):\n",
    "        \"\"\"Initialize analyzer with path to augmented dataset.\"\"\"\n",
    "        self.file_path = file_path\n",
    "        with open(file_path, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "        self.df = pd.DataFrame(self.data)\n",
    "        \n",
    "    def find_exact_duplicates(self):\n",
    "        \"\"\"Find completely identical profiles.\"\"\"\n",
    "        duplicates = self.df.duplicated(keep='first')\n",
    "        return {\n",
    "            'total_duplicates': duplicates.sum(),\n",
    "            'duplicate_indices': duplicates[duplicates].index.tolist()\n",
    "        }\n",
    "    \n",
    "    def find_similar_descriptions(self, threshold=0.85):\n",
    "        \"\"\"Find profiles with very similar descriptions.\"\"\"\n",
    "        similar_pairs = []\n",
    "        descriptions = self.df['donor_description'].dropna().tolist()\n",
    "        \n",
    "        for i in range(len(descriptions)):\n",
    "            for j in range(i + 1, len(descriptions)):\n",
    "                similarity = SequenceMatcher(None, \n",
    "                                          descriptions[i], \n",
    "                                          descriptions[j]).ratio()\n",
    "                if similarity > threshold:\n",
    "                    similar_pairs.append({\n",
    "                        'index1': i,\n",
    "                        'index2': j,\n",
    "                        'similarity': similarity,\n",
    "                        'text1': descriptions[i][:100],\n",
    "                        'text2': descriptions[j][:100]\n",
    "                    })\n",
    "        return similar_pairs\n",
    "    \n",
    "    def check_height_weight_validity(self):\n",
    "        \"\"\"Check for unrealistic height/weight combinations.\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        def parse_height(height):\n",
    "            try:\n",
    "                return float(height.replace(\"'\", \".\"))\n",
    "            except:\n",
    "                return None\n",
    "        \n",
    "        def parse_weight(weight):\n",
    "            try:\n",
    "                return float(weight)\n",
    "            except:\n",
    "                return None\n",
    "        \n",
    "        # Convert height and weight to numeric\n",
    "        self.df['height_num'] = self.df['height'].apply(parse_height)\n",
    "        self.df['weight_num'] = self.df['weight'].apply(parse_weight)\n",
    "        \n",
    "        # Check for unrealistic values\n",
    "        height_issues = self.df[\n",
    "            (self.df['height_num'] < 4.5) | \n",
    "            (self.df['height_num'] > 7.0)\n",
    "        ].index.tolist()\n",
    "        \n",
    "        weight_issues = self.df[\n",
    "            (self.df['weight_num'] < 80) | \n",
    "            (self.df['weight_num'] > 300)\n",
    "        ].index.tolist()\n",
    "        \n",
    "        # Check height-weight ratio\n",
    "        bmi_issues = self.df[\n",
    "            (self.df['weight_num'] / ((self.df['height_num'] * 0.3048) ** 2) < 16) |\n",
    "            (self.df['weight_num'] / ((self.df['height_num'] * 0.3048) ** 2) > 45)\n",
    "        ].index.tolist()\n",
    "        \n",
    "        return {\n",
    "            'height_issues': height_issues,\n",
    "            'weight_issues': weight_issues,\n",
    "            'bmi_issues': bmi_issues\n",
    "        }\n",
    "    \n",
    "    def check_text_quality(self):\n",
    "        \"\"\"Check for potential issues in text descriptions.\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        # Patterns to check\n",
    "        patterns = {\n",
    "            'very_short': lambda x: len(x.split()) < 10,\n",
    "            'very_long': lambda x: len(x.split()) > 200,\n",
    "            'repeated_words': lambda x: len(re.findall(r'\\b(\\w+)(?:\\s+\\1\\b)+', x)) > 0,\n",
    "            'unusual_punctuation': lambda x: len(re.findall(r'[!?]{2,}', x)) > 0\n",
    "        }\n",
    "        \n",
    "        text_issues = defaultdict(list)\n",
    "        for idx, desc in enumerate(self.df['donor_description'].dropna()):\n",
    "            for pattern_name, pattern_func in patterns.items():\n",
    "                if pattern_func(desc):\n",
    "                    text_issues[pattern_name].append(idx)\n",
    "        \n",
    "        return dict(text_issues)\n",
    "    \n",
    "    def check_missing_values(self):\n",
    "        \"\"\"Check for missing values in important fields.\"\"\"\n",
    "        return {\n",
    "            'total_missing': self.df.isnull().sum().to_dict(),\n",
    "            'profiles_with_missing': len(self.df[self.df.isnull().any(axis=1)])\n",
    "        }\n",
    "    \n",
    "    def analyze_synthetic_distribution(self):\n",
    "        \"\"\"Analyze distribution of original vs synthetic profiles.\"\"\"\n",
    "        synthetic_count = len(self.df[self.df['source'] == 'synthetic'])\n",
    "        original_count = len(self.df[self.df['source'] == 'original'])\n",
    "        \n",
    "        return {\n",
    "            'synthetic_profiles': synthetic_count,\n",
    "            'original_profiles': original_count,\n",
    "            'synthetic_ratio': synthetic_count / (original_count + synthetic_count),\n",
    "            'distribution_by_ethnicity': self.df.groupby(['ethnic_background', 'source']).size().to_dict()\n",
    "        }\n",
    "    \n",
    "    def generate_full_report(self):\n",
    "        \"\"\"Generate comprehensive quality report.\"\"\"\n",
    "        report = {\n",
    "            'exact_duplicates': self.find_exact_duplicates(),\n",
    "            'similar_descriptions': self.find_similar_descriptions(),\n",
    "            'physical_issues': self.check_height_weight_validity(),\n",
    "            'text_issues': self.check_text_quality(),\n",
    "            'missing_values': self.check_missing_values(),\n",
    "            'synthetic_analysis': self.analyze_synthetic_distribution()\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "\n",
    "def print_report(report):\n",
    "    \"\"\"Print formatted report.\"\"\"\n",
    "    print(\"\\n=== DATA QUALITY REPORT ===\\n\")\n",
    "    \n",
    "    print(\"EXACT DUPLICATES:\")\n",
    "    print(f\"Found {report['exact_duplicates']['total_duplicates']} exact duplicates\")\n",
    "    \n",
    "    print(\"\\nSIMILAR DESCRIPTIONS:\")\n",
    "    similar_count = len(report['similar_descriptions'])\n",
    "    print(f\"Found {similar_count} pairs of very similar descriptions\")\n",
    "    if similar_count > 0:\n",
    "        print(\"\\nExample of similar pair:\")\n",
    "        print(f\"Text 1: {report['similar_descriptions'][0]['text1']}\")\n",
    "        print(f\"Text 2: {report['similar_descriptions'][0]['text2']}\")\n",
    "        print(f\"Similarity: {report['similar_descriptions'][0]['similarity']:.2f}\")\n",
    "    \n",
    "    print(\"\\nPHYSICAL ATTRIBUTE ISSUES:\")\n",
    "    print(f\"Height issues: {len(report['physical_issues']['height_issues'])}\")\n",
    "    print(f\"Weight issues: {len(report['physical_issues']['weight_issues'])}\")\n",
    "    print(f\"BMI issues: {len(report['physical_issues']['bmi_issues'])}\")\n",
    "    \n",
    "    print(\"\\nTEXT QUALITY ISSUES:\")\n",
    "    for issue, indices in report['text_issues'].items():\n",
    "        print(f\"{issue}: {len(indices)} instances\")\n",
    "    \n",
    "    print(\"\\nMISSING VALUES:\")\n",
    "    print(f\"Profiles with any missing values: {report['missing_values']['profiles_with_missing']}\")\n",
    "    \n",
    "    print(\"\\nSYNTHETIC VS ORIGINAL:\")\n",
    "    synth = report['synthetic_analysis']\n",
    "    print(f\"Original profiles: {synth['original_profiles']}\")\n",
    "    print(f\"Synthetic profiles: {synth['synthetic_profiles']}\")\n",
    "    print(f\"Synthetic ratio: {synth['synthetic_ratio']:.2f}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    analyzer = DataQualityAnalyzer(\"/Users/cindylinsf/Documents/CCI/THESIS/Msc_Thesis_Project_Files/data/generated/augmented_dataset/augmented_dataset.json\")\n",
    "    report = analyzer.generate_full_report()\n",
    "    print_report(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
